{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "URL_PDF_Search.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ofJ_DGpQRFqX",
        "lIwUY2J0jxY7",
        "qxxbyduuVG1y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofJ_DGpQRFqX"
      },
      "source": [
        "# A. Overview\n",
        "The general idea of this program is to run a control+F on a bunch of PDFs from a webpage, with multiple search terms. If you're looking through notice and comment documents (like [these](https://www.copyright.gov/1201/2018/comments-031418/)) or any webpage storing PDFs, it can identify all the PDFs on a given page (generally) and then search all of them for terms you specify. \n",
        "\n",
        "***It's important to note that in order for a PDF to be readable by the program, the PDF has to be stored such that when a mouse hovers over the PDF's link on Google Chrome, the url that appears in the bottom left corner of the browser ends in \".pdf\".***\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMt-nQy9ZDJd"
      },
      "source": [
        "# B. How to Use\n",
        "\n",
        "1.   Scroll down to the User Inputs in section E, and enter a URL (where the PDFs of interest are located) in the user inputs field on the right side of the page.\n",
        "\n",
        "2.   Enter keyword(s) or phrase(s) for which you wish to search in a pipe-delimited (\"|\") format (or other selected delimiter). \n",
        "> e.g. \"word1|phrase of interest 1|word2\"\n",
        "\n",
        "    There's no limit to how many words you can search, so it can pay off to be creative and comprehensive with search terms, like including both \"FCC\" and \"Federal Communications Commission\"\n",
        "\n",
        "3. On the toolbar at the top of the page, click Runtime -> Run All (or press Ctrl + F9)\n",
        "\n",
        "4. You can scroll to the bottom of the page to get updates on what percentage of PDFs have been read, and when the program is complete, it will prompt a download of a CSV file that lists all references to the keywords with a bit of the surrounding text, and links to the relevant PDFs.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIwUY2J0jxY7"
      },
      "source": [
        "## C. Keyword Tips\n",
        "Though the pipe may be unfamiliar, this allows searching for the occasional instances in which commas or semi-colons could matter for the search. \n",
        "\n",
        "#### Note that spacing matters: \n",
        "\n",
        "A keyword entry of \"cat|keyword2\" would flag instances of cat, category and concatenate.\n",
        "\n",
        "If an extra space were inserted at the beginning, a keyword entry of \" cat|keyword2\" would return cat and category but **not** concatenate since there isn't a space before cat in concatenate.\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxxbyduuVG1y"
      },
      "source": [
        "##D. Important Caveats\n",
        "1. The false-negative rate should be about the same as Control+F, so if opening each of these documents and Ctrl+Fing through would be precise enough for your purposes, this should do about as well as that, but faster and more easily. \n",
        "\n",
        "2. There will be some false positives. It is using computer vision, and occasionally of fuzzy scans, so sometimes it will return screwy things like blanks or bizarre characters or visually similar but unwanted results (e.g. it seems to mistake the dots in i's as periods sometimes). The idea isn't for the program to read for you but to identify at a high rate portions of documents that could be of interest, so hopefully it can do that in spite of these irregularities.\n",
        "\n",
        "\n",
        "3. Because this runs on Google's cloud, it can be slightly slow (especially downloading a package at the start of each session). If you have Python set up on your computer, it should be straightforward to download this code (File->Download), and it should run significantly faster locally, and at the very least without the time cost of installing packages (as oppsed to importing them) at the start of every session. \n",
        "\n",
        "4. The tool has worked well on government pages, Google search, Google Scholar, etc. There are, however, occasionally PDFs which the program can't open (PDF urls with spaces throw it off, for example), which is frustrating. The output at the bottom of the page will tell you which files were not read. As time permits development may iron out these issues.\n",
        "\n",
        "The search is NOT case-sensitive.\n",
        "\n",
        "If there is interest in a version that could run through a local folder rather than a webpage, that would be feasible too.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj0BKyZTmQQc"
      },
      "source": [
        "#E. User inputs\n",
        "Insert the url to be searched for PDFs and then a list of keywords or phrases separated by pipes (\"|\", usually on the backslash key). Other delimiters can be selected if desired.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HroPNI9_josN",
        "cellView": "form"
      },
      "source": [
        "url = \"https://www.copyright.gov/1201/2018/comments-031418/\" #@param {type:\"string\"}\n",
        "delimiter = \"|\" #@param [\"|\", \",\", \";\"]\n",
        "Keywords = \"FCC|Federal Communcations Commission|FAA|Environmental Protection Agency|\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn19YQsnmZRD"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqvmj3jlmq_j"
      },
      "source": [
        "# This part takes a bit (maybe a minute), but needs to run at the start of each session; once you have done one run, however, it won't take that long (1-2 seconds) for each subsequent run in the same session\n",
        "!pip install pdfminer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjkFmFguKXfL"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "import time\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import urljoin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j5fR8mdY-pv"
      },
      "source": [
        "def pull_keywords(keywords, text, filename):\n",
        "    out_df = pd.DataFrame(columns=['Keyword','Reference','File'])\n",
        "    for keyword in keywords:\n",
        "        k = keyword.lower()\n",
        "        t = text.lower()\n",
        "        res = [i.start() for i in re.finditer(k, t)]\n",
        "        if len(res)>0:\n",
        "            print(keyword +' found '+str(len(res))+' time(s) in '+str(filename))\n",
        "        for i in res:\n",
        "            tmp = pd.DataFrame({'Keyword':[keyword],'Reference':[text[i-75:i+75].replace(\"\\n\",\" \")],'File':[filename]})\n",
        "            out_df = out_df.append(tmp)\n",
        "    return out_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kelbbX8YAhQ"
      },
      "source": [
        "def pdf_to_text(in_file):\n",
        "    output_string = io.StringIO()\n",
        "    parser = PDFParser(in_file)\n",
        "    doc = PDFDocument(parser)\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    for page in PDFPage.create_pages(doc):\n",
        "        interpreter.process_page(page)\n",
        "    text = output_string.getvalue()   \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U02hCjN-VOIS"
      },
      "source": [
        "def pull_pdfs(url, keywords, outfile_name):\n",
        "    print('Searching for PDFs at '+url)\n",
        "    print('Searching for these terms: ', Keywords) \n",
        "    \n",
        "    pdf_url_list = []\n",
        "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3538.102 Safari/537.36 Edge/18.19582\"}\n",
        "    response = requests.get(url,headers=headers)\n",
        "#--Collecting PDFs on provided URL (2 versions)\n",
        "    ## A - Handle Google search results since they work differently\n",
        "    if url[:29] == 'https://www.google.com/search':\n",
        "        soup = BeautifulSoup(response.content)\n",
        "        for link in soup.select('a[href*=\".pdf\"]'):\n",
        "            i = re.split(\":(?=http)\",link[\"href\"].replace(\"/url?q=\",\"\"))[0]\n",
        "            out = i[:i.find('.pdf')+4]\n",
        "            if out[-4:]== '.pdf':\n",
        "                pdf_url_list.append(out)\n",
        "            \n",
        "    ## B - Address all non-Google search URLs\n",
        "    else:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        for link in soup.select(\"a[href$='.pdf']\"):\n",
        "         #below quirk could be problematic at some point -- could be built into exception handling\n",
        "            if link['href'][0] == 'h':\n",
        "                pdf_url_list.append(link['href'])\n",
        "\n",
        "#--------Resume after Google split\n",
        "    if len(pdf_url_list) == 0:\n",
        "        return ('No PDFs were able to be found on this site.')\n",
        "\n",
        "    denom = len(pdf_url_list)\n",
        "    print('Number of PDFs found at this site: ', denom)\n",
        "    count = 1\n",
        "    out_df = pd.DataFrame(columns=['Keyword','Reference','File'])\n",
        "    not_read = []\n",
        "\n",
        "    for pdf_url in pdf_url_list: \n",
        "        try:\n",
        "            open = requests.get(pdf_url).content\n",
        "            pdf = io.BytesIO(open)\n",
        "            txt = pdf_to_text(pdf)\n",
        "            results = pull_keywords(keywords=keywords, text = txt, filename = pdf_url)\n",
        "            out_df = out_df.append(results)\n",
        "        except:\n",
        "            print(\"This file was not read; that could be because it's too big or because there is a login step. \", pdf_url)\n",
        "            not_read.append(pdf_url)\n",
        "            pass\n",
        "\n",
        "        print(str(round((count/denom)*100,1))+ '% of PDFs have been processed.')\n",
        "        count += 1\n",
        "    out_df.to_csv(outfile_name, index=False)\n",
        "\n",
        "    try:\n",
        "        files.download(outfile_name)\n",
        "    except ModuleNotFoundError:\n",
        "        pass\n",
        "    \n",
        "    if len(not_read)>0:\n",
        "        print('The program failed to read these PDFs:')\n",
        "        for i in not_read:\n",
        "            print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKxEa46Mnv_F"
      },
      "source": [
        "# Structuring input parameters a bit\n",
        "if len(url) == 0:\n",
        "    print('No URL provided; setting to test URL')\n",
        "    url = 'https://www.copyright.gov/1201/2014/petitions/'\n",
        "\n",
        "if url[0] in [\"'\",\"\\\"\"]:\n",
        "    url = url[1:]\n",
        "if url[-1] in [\"'\",\"\\\"\"]:\n",
        "    url = url[:-1]\n",
        "\n",
        "if len(delimiter) == 0:\n",
        "    delimiter = \"|\"\n",
        "\n",
        "if len(Keywords) == 0:\n",
        "    print('No keywords detected; setting to test keywords')\n",
        "    Keywords = ['marginal cost', 'endogenous growth']\n",
        "elif isinstance(Keywords,list) == False:\n",
        "    Keywords = Keywords.split(delimiter)\n",
        "else:\n",
        "    print('User input of Keywords accepted')\n",
        "\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "Output = 'url_pdf_index'+timestr+'.csv'\n",
        "pull_pdfs(url, keywords = Keywords, outfile_name=Output)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}